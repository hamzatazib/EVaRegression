{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:22.128000-05:00",
     "start_time": "2019-12-03T21:01:22.113Z"
    }
   },
   "outputs": [],
   "source": [
    "using CSV, Random, StatsBase, LinearAlgebra, Distributions, DataFrames, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:22.576000-05:00",
     "start_time": "2019-12-03T21:01:22.562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "train = CSV.read(\"data/housing_train.csv\")\n",
    "test = CSV.read(\"data/housing_test.csv\")\n",
    "\n",
    "#From DataFrames to Matrices\n",
    "#input data\n",
    "X = convert(Matrix, train[:,1:13])\n",
    "X_test = convert(Matrix, test[:,1:13])\n",
    "\n",
    "#demand\n",
    "Y = train[:, 14]\n",
    "Y_test = test[:,14]\n",
    "\n",
    "n, p = size(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:22.946000-05:00",
     "start_time": "2019-12-03T21:01:22.774Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18×13 Array{Float64,2}:\n",
       "  1.27203    -1.47295     1.28473     …   4.45659    -1.73102    -2.16122 \n",
       " -1.48316     0.258967   -0.443175        2.28431     0.668575   -0.628678\n",
       " -3.12591    -2.6333      0.907485       -1.11286    -1.82937     2.7136  \n",
       " -3.60461    -2.95503    -0.847187       -0.735588   -0.397701    3.2896  \n",
       "  2.52388     0.517058   -1.87484         1.21371    -0.667525    1.51315 \n",
       " -0.797983    6.63083     0.564393    …   0.545131   -0.804266   -1.95628 \n",
       " -1.07108     3.89539    -3.16024        -1.29292    -0.889224   -1.29641 \n",
       "  0.863789   -0.731209   -0.680703        3.70939    -6.92431     0.529326\n",
       "  1.23695    -0.661774    2.01435         0.287445   -2.14693    -0.586912\n",
       "  1.68957    -1.07031     1.94114        -0.921036   -0.259276    1.5927  \n",
       " -0.48521    -0.116656    0.746018    …  -1.64007    -0.48752     0.429787\n",
       " -0.999055   -0.210989   -0.104619       -2.00987     0.0766653  -0.939519\n",
       "  1.73845     0.0697995  -2.89245        -2.59572     0.0634938  -2.15936 \n",
       " -1.28739     2.12363    -1.66762        -1.48383     0.958572   -2.17312 \n",
       "  2.32152    -1.85445     3.87076        -0.0524614   3.65711    -4.33374 \n",
       " -0.904056   -1.8839     -2.34782     …  -0.820168   -0.391332    0.181388\n",
       "  0.0824879  -0.785693   -1.19377         0.137466    1.41905    -1.01831 \n",
       " -0.654694    1.44688    -5.92639e-5     -0.627366    0.352027    0.174988"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add Outliers to train data\n",
    "percent_outliers = 5\n",
    "idx_outliers = shuffle(1:n)[1:Int(round(percent_outliers/100*n))]\n",
    "\n",
    "σ = 1.5; μ=0\n",
    "X[idx_outliers,:] = (μ .+ (σ.*Random.randn((size(idx_outliers)[1],p)))) + X[idx_outliers,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVaR Regression Regularized\n",
    "$$\n",
    "\\min_{s, w} \\lambda ||w||^2 + s \\log \\left(\\frac{1}{n\\alpha} \\sum_{i=1}^n \\exp \\left(\\frac{(Y_i - w^\\top X_i)^2}{s}\\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### $\\ell_2$ EVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:30.487000-05:00",
     "start_time": "2019-12-03T21:01:30.460Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇wobjective_ℓ2 (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function objective_ℓ2(X,Y,s,w,α, λ)\n",
    "    \"\"\"\n",
    "    α: fraction of data that constitutes the training set ∈ [0,1]\n",
    "    \"\"\"\n",
    "    n = size(X)[1]\n",
    "    return λ*dot(w, w) + s*log(sum(exp(1/s*(dot(w,X[i,:])-Y[i])^2) for i=1:n)/(n*α))\n",
    "end\n",
    "\n",
    "function ∇sobjective_ℓ2(X,Y,s,w,α,λ) # Not affected bY λ but still putting it for function homogeneity issue\n",
    "    n = size(X)[1]\n",
    "    return log(sum(exp(1/s*(dot(w,X[i,:])-Y[i])^2) for i=1:n)/(n*α)) - sum(((dot(w,X[i,:])-Y[i])^2)*exp(1/s*(dot(w,X[i,:])-Y[i])^2) for i=1:n)/sum(exp(1/s*(dot(w,X[i,:])-Y[i])^2) for i=1:n)/s\n",
    "end\n",
    "\n",
    "function ∇wobjective_ℓ2(X,Y,s,w,α,λ)\n",
    "    n = size(X)[1]\n",
    "    return 2*λ*w + 2*sum(((dot(w,X[i,:])-Y[i]))*exp(1/s*(dot(w,X[i,:])-Y[i])^2)*X[i,:] for i=1:n)/sum(exp(1/s*(dot(w,X[i,:])-Y[i])^2) for i=1:n)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:30.765000-05:00",
     "start_time": "2019-12-03T21:01:30.754Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_descent_EVaR_ℓ2 (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gradient_descent_EVaR_ℓ2(X, Y, s_0, w_0, α, λ, c_s, c_w, ε)\n",
    "    \"\"\"\n",
    "    c_s: constant learning rate associated to the ∇ w.r.t s\n",
    "    c_w: constant learning rate associated to the ∇ w.r.t w\n",
    "    ε: \n",
    "    \"\"\"\n",
    "    #println(objective_ℓ2(X,Y,s_0,w_0,α,λ))\n",
    "    list_f = [objective_ℓ2(X,Y,s_0,w_0,α,λ)]\n",
    "    s=s_0; w=w_0\n",
    "    n_grad = dot(∇wobjective_ℓ2(X,Y,s,w,α,λ), ∇wobjective_ℓ2(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ2(X,Y,s,w,α,λ),∇sobjective_ℓ2(X,Y,s,w,α,λ))\n",
    "    k=1\n",
    "    while n_grad > ε\n",
    "        if k == 30000\n",
    "            break\n",
    "        end\n",
    "        s = s - c_s*∇sobjective_ℓ2(X,Y,s,w,α,λ)\n",
    "        s = max(0.000000001,s)\n",
    "        w = w - c_w*∇wobjective_ℓ2(X,Y,s,w,α,λ)\n",
    "        n_grad = dot(∇wobjective_ℓ2(X,Y,s,w,α,λ), ∇wobjective_ℓ2(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ2(X,Y,s,w,α,λ),∇sobjective_ℓ2(X,Y,s,w,α,λ))\n",
    "        push!(list_f, objective_ℓ2(X,Y,s,w,α,λ))\n",
    "        k = k+1\n",
    "    end\n",
    "    list_f, s, w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:30.946000-05:00",
     "start_time": "2019-12-03T21:01:30.935Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nesterov_gradient_descent_EVaR_ℓ2 (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented\n",
    "    function nesterov_gradient_descent_EVaR_ℓ2(X, Y, s_0, w_0, α, λ, c_s, c_w, ε, μ)\n",
    "            \"\"\"\n",
    "        c_s: constant learning rate associated to the ∇ w.r.t s\n",
    "        c_w: constant learning rate associated to the ∇ w.r.t w\n",
    "        ε: stopping gradient norm \n",
    "        μ: momentum\n",
    "        \"\"\"\n",
    "        #println(objective_ℓ2(X,Y,s_0,w_0,α,λ))\n",
    "        list_f = [objective_ℓ2(X,Y,s_0,w_0,α,λ)]\n",
    "        s=s_0; w=w_0\n",
    "        n_grad = dot(∇wobjective_ℓ2(X,Y,s,w,α,λ), ∇wobjective_ℓ2(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ2(X,Y,s,w,α,λ),∇sobjective_ℓ2(X,Y,s,w,α,λ))\n",
    "        k=1\n",
    "        while n_grad > ε\n",
    "            if k == 30000\n",
    "                break\n",
    "            end\n",
    "            s = (s - c_s*∇sobjective_ℓ2(X,Y,s,w,α,λ))*(1+μ) - μ*s\n",
    "            s = max(0.000000001,s)\n",
    "            w = (w - c_w*∇wobjective_ℓ2(X,Y,s,w,α,λ))*(1+μ) - μ*w\n",
    "            n_grad = dot(∇wobjective_ℓ2(X,Y,s,w,α,λ), ∇wobjective_ℓ2(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ2(X,Y,s,w,α,λ),∇sobjective_ℓ2(X,Y,s,w,α,λ))\n",
    "            push!(list_f, objective_ℓ2(X,Y,s,w,α,λ))\n",
    "            k = k+1\n",
    "        end\n",
    "        list_f, s, w\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### $\\ell_1$ EVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:32.261000-05:00",
     "start_time": "2019-12-03T21:01:32.240Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇wobjective_ℓ1 (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function objective_ℓ1(X,Y,s,w,α, λ)\n",
    "    \"\"\"\n",
    "    α: fraction of data that constitutes the training set ∈ [0,1]\n",
    "    \"\"\"\n",
    "    n = size(X)[1]\n",
    "    return λ*dot(w, w) + s*log(sum(exp(1/s*abs(dot(w,X[i,:])-Y[i])) for i=1:n)/(n*α))\n",
    "end\n",
    "\n",
    "function ∇sobjective_ℓ1(X,Y,s,w,α,λ) # Not affected bY λ but still putting it for function homogeneity issue\n",
    "    n = size(X)[1]\n",
    "    return log(sum(exp(1/s*abs(dot(w,X[i,:])-Y[i])) for i=1:n)/(n*α)) - sum(abs(dot(w,X[i,:])-Y[i])*exp(1/s*(abs(dot(w,X[i,:])-Y[i]))) for i=1:n)/sum(exp(1/s*abs(dot(w,X[i,:])-Y[i])) for i=1:n)/s\n",
    "end\n",
    "\n",
    "function ∇wobjective_ℓ1(X,Y,s,w,α,λ)\n",
    "    n = size(X)[1]\n",
    "    return 2*λ*w + sum(sign.((dot(w,X[i,:])-Y[i]))*exp(1/s*(abs(dot(w,X[i,:])-Y[i])))*X[i,:] for i=1:n)/sum(exp(1/s*(abs(dot(w,X[i,:])-Y[i]))) for i=1:n)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:33.092000-05:00",
     "start_time": "2019-12-03T21:01:33.082Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_descent_EVaR_ℓ1 (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gradient_descent_EVaR_ℓ1(X, Y, s_0, w_0, α, λ, c_s, c_w, ε)\n",
    "    \"\"\"\n",
    "    c_s: constant learning rate associated to the ∇ w.r.t s\n",
    "    c_w: constant learning rate associated to the ∇ w.r.t w\n",
    "    ε: \n",
    "    \"\"\"\n",
    "    #println(objective_ℓ1(X,Y,s_0,w_0,α,λ))\n",
    "    list_f = [objective_ℓ1(X,Y,s_0,w_0,α,λ)]\n",
    "    s=s_0; w=w_0\n",
    "    n_grad = dot(∇wobjective_ℓ1(X,Y,s,w,α,λ), ∇wobjective_ℓ1(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ1(X,Y,s,w,α,λ),∇sobjective_ℓ1(X,Y,s,w,α,λ))\n",
    "    k=1\n",
    "    while n_grad > ε\n",
    "        s = s - c_s*∇sobjective_ℓ1(X,Y,s,w,α,λ)\n",
    "        s = max(0.000000001,s)\n",
    "        w = w - c_w*∇wobjective_ℓ1(X,Y,s,w,α,λ)\n",
    "        n_grad = dot(∇wobjective_ℓ1(X,Y,s,w,α,λ), ∇wobjective_ℓ1(X,Y,s,w,α,λ)) + dot(∇sobjective_ℓ1(X,Y,s,w,α,λ),∇sobjective_ℓ1(X,Y,s,w,α,λ))\n",
    "        push!(list_f, objective_ℓ1(X,Y,s,w,α,λ))\n",
    "        if k == 30000\n",
    "            break\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    list_f, s, w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:01:34.139000-05:00",
     "start_time": "2019-12-03T21:01:34.136Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nesterov_gradient_descent_EVaR_ℓ1 (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented\n",
    "function nesterov_gradient_descent_EVaR_ℓ1()\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Hyperparameter tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T22:19:50.596000-05:00",
     "start_time": "2019-12-04T03:19:41.822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_MAE (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit_least_squares(X_i, Y_i, w_opt)\n",
    "    return (Y_i - dot(w_opt, X_i))^2\n",
    "end\n",
    "\n",
    "function fit_least_absolute_values(X_i, Y_i, w_opt)\n",
    "    return abs(Y_i - dot(w_opt, X_i))\n",
    "end\n",
    "\n",
    "function calculate_MSE(X, Y, w_opt)\n",
    "    return mean((X*w_opt-Y).^2)\n",
    "end\n",
    "\n",
    "function calculate_MAE(X, Y, w_opt)\n",
    "    return mean(broadcast(abs, X*w_opt-Y))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T22:20:07.274000-05:00",
     "start_time": "2019-12-04T03:20:07.106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_training_and_validation_indices (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_training_and_validation_indices(X, Y, w_opt, train_is_worse, k)\n",
    "    \"\"\"\n",
    "    train_is_worse: boolean, true if the training set contains the worst errors, false if it's the validation\n",
    "    k: number of observations in the training set\n",
    "    \"\"\"\n",
    "    n,p = size(X)\n",
    "    least_squares_df = DataFrame()\n",
    "    least_squares_df.Obs_Index = 1:n\n",
    "    least_squares_df.LS_Value = [fit_least_squares(X[i,:], Y[i], w_opt) for i=1:n]\n",
    "    least_squares_sorted = sort!(least_squares_df, :LS_Value, rev=true)\n",
    "    if train_is_worse == true\n",
    "        train_indices = least_squares_sorted[1:k, :Obs_Index]\n",
    "        val_indices = least_squares_sorted[k+1:n, :Obs_Index]\n",
    "    else\n",
    "        val_indices = least_squares_sorted[1:k, :Obs_Index]\n",
    "        train_indices = least_squares_sorted[k+1:n, :Obs_Index]\n",
    "    end\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Regressions (Housing Dataset) - 70/30 Train/Val splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVaRegression ℓ2 with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:19:55.641000-05:00",
     "start_time": "2019-12-03T21:01:57.814Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0.0\t c_s_EVaR=0.0001\t c_w_EVaR=0.0001\t MSE=0.014455699425704602\n",
      "λ=0.0\t c_s_EVaR=0.001\t c_w_EVaR=0.0001\t MSE=0.013959216578132911\n",
      "λ=0.0\t c_s_EVaR=0.01\t c_w_EVaR=0.0001\t MSE=0.013942508622192111\n",
      "λ=0.0\t c_s_EVaR=0.1\t c_w_EVaR=0.0001\t MSE=0.013941329327492747\n",
      "λ=0.01\t c_s_EVaR=0.001\t c_w_EVaR=0.0001\t MSE=0.01385217527775273\n",
      "λ=0.01\t c_s_EVaR=0.01\t c_w_EVaR=0.0001\t MSE=0.013828770847216319\n",
      "λ=0.01\t c_s_EVaR=0.1\t c_w_EVaR=0.0001\t MSE=0.013827220667039185\n",
      "λ=0.1\t c_s_EVaR=0.0001\t c_w_EVaR=0.0001\t MSE=0.013448002066065437\n",
      "λ=0.1\t c_s_EVaR=0.001\t c_w_EVaR=0.0001\t MSE=0.012564338553408928\n",
      "λ=0.1\t c_s_EVaR=0.1\t c_w_EVaR=0.0001\t MSE=0.012563205022912237\n",
      "λ=0.2\t c_s_EVaR=0.001\t c_w_EVaR=0.0001\t MSE=0.0120642119408125\n",
      "λ=0.2\t c_s_EVaR=0.01\t c_w_EVaR=0.0001\t MSE=0.012057988636107714\n",
      "λ=0.2\t c_s_EVaR=0.1\t c_w_EVaR=0.0001\t MSE=0.012055680799439028\n",
      "λ=1.0\t c_s_EVaR=0.001\t c_w_EVaR=0.0001\t MSE=0.011671703525701209\n",
      "λ=1.0\t c_s_EVaR=0.01\t c_w_EVaR=0.001\t MSE=0.011670581135417886\n",
      "λ=1.0\t c_s_EVaR=0.1\t c_w_EVaR=0.01\t MSE=0.011669747324927022\n",
      "Best λ_EVaR = 1.0\t c_s_EVaR=0.1\t c_w_EVaR=0.01\n",
      "Fitted EVaRegression ℓ_2 with best possible λ_EVaR=1.0  c_s_EVaR=0.1  c_w_EVaR=0.01\n"
     ]
    }
   ],
   "source": [
    "for train_prop in [0.7,0.6,0.5]\n",
    "\n",
    "    #### Finding best EVaRegression l2 with random start\n",
    "\n",
    "    n, p = size(X);   k = floor(Int, train_prop*n);   α = k/n;   ε=0.001\n",
    "    # Initial values, we use RLS weights as warm start\n",
    "    w_0 = ones(p)./100\n",
    "    s_0 = 0.9\n",
    "\n",
    "\n",
    "    best_λ_EVaR = 0\n",
    "    best_c_s_EVaR = 0\n",
    "    best_c_w_EVaR = 0\n",
    "    best_MSE_EVaR = 1000000\n",
    "    for λ_EVaR in [0, 0.01, 0.1, 0.2, 1, 10, 20, 50, 100]\n",
    "        for c_s_EVaR in [0.0001, 0.001, 0.01, 0.1]\n",
    "            for c_w_EVaR in [0.0001, 0.001, 0.01, 0.1]\n",
    "                list_f_opt, s_opt, w_opt_EVaR = gradient_descent_EVaR_ℓ2(X,Y,s_0,w_0,α,λ_EVaR,c_s_EVaR,c_w_EVaR,ε)\n",
    "                train_indices_EVaR, val_indices_EVaR = get_training_and_validation_indices(\n",
    "                    X, Y, w_opt_EVaR, true, k\n",
    "                )\n",
    "                X_train_EVaR, Y_train_EVaR = X[train_indices_EVaR, :], Y[train_indices_EVaR]\n",
    "                X_val_EVaR, Y_val_EVaR = X[val_indices_EVaR, :], Y[val_indices_EVaR]\n",
    "                MSE_EVaR = calculate_MSE(X_val_EVaR, Y_val_EVaR, w_opt_EVaR)\n",
    "                if MSE_EVaR < best_MSE_EVaR\n",
    "                    best_λ_EVaR = λ_EVaR\n",
    "                    best_c_s_EVaR = c_s_EVaR\n",
    "                    best_c_w_EVaR = c_w_EVaR\n",
    "                    best_MSE_EVaR = MSE_EVaR\n",
    "                    println(\"λ=\", λ_EVaR, \"\\t c_s_EVaR=\", c_s_EVaR, \"\\t c_w_EVaR=\", c_w_EVaR, \"\\t MSE=\", MSE_EVaR)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    println(\"Best λ_EVaR = \", best_λ_EVaR, \"\\t c_s_EVaR=\", best_c_s_EVaR, \"\\t c_w_EVaR=\", best_c_w_EVaR)\n",
    "\n",
    "    # Fitting Stable regression with best λ\n",
    "    list_f_opt, s_opt, w_opt_EVaR = gradient_descent_EVaR_ℓ2(\n",
    "        X, Y, s_0, w_0, α, best_λ_EVaR, best_c_s_EVaR, best_c_w_EVaR, ε\n",
    "    )\n",
    "    train_indices_EVaR, val_indices_EVaR = get_training_and_validation_indices(\n",
    "        X, Y, w_opt_EVaR, true, k\n",
    "    )\n",
    "    X_train_EVaR, Y_train_EVaR = X[train_indices_EVaR, :], Y[train_indices_EVaR]\n",
    "    X_val_EVaR, Y_val_EVaR = X[val_indices_EVaR, :], Y[val_indices_EVaR]\n",
    "    println(\"Fitted EVaRegression ℓ_2 with best possible λ_EVaR=\", best_λ_EVaR, \n",
    "        \"  c_s_EVaR=\", best_c_s_EVaR, \"  c_w_EVaR=\", best_c_w_EVaR)\n",
    "\n",
    "        #### Finding best EVaRegression l1 with random warm start\n",
    "\n",
    "    n, p = size(X);   k = floor(Int, train_prop*n);   α = k/n;   ε=0.001\n",
    "    # Initial values, we use RLS weights as warm start\n",
    "    w_0 = ones(p)./100\n",
    "    s_0 = 0.9\n",
    "\n",
    "\n",
    "    best_λ_EVaR_ℓ1 = 0\n",
    "    best_c_s_EVaR_ℓ1 = 0\n",
    "    best_c_w_EVaR_ℓ1 = 0\n",
    "    best_MSE_EVaR_ℓ1 = 1000000\n",
    "    for λ_EVaR_ℓ1 in [0, 0.01, 0.1, 0.2, 1, 10, 20, 50, 100]\n",
    "        for c_s_EVaR_ℓ1 in [0.0001, 0.001, 0.01, 0.1]\n",
    "            for c_w_EVaR_ℓ1 in [0.0001, 0.001, 0.01, 0.1]\n",
    "                list_f_opt, s_opt, w_opt_EVaR_ℓ1 = gradient_descent_EVaR_ℓ1(X,Y,s_0,w_0,α,λ_EVaR_ℓ1,c_s_EVaR_ℓ1,c_w_EVaR_ℓ1,ε)\n",
    "                train_indices_EVaR_ℓ1, val_indices_EVaR_ℓ1 = get_training_and_validation_indices(\n",
    "                    X, Y, w_opt_EVaR_ℓ1, true, k\n",
    "                )\n",
    "                X_train_EVaR_ℓ1, Y_train_EVaR_ℓ1 = X[train_indices_EVaR_ℓ1, :], Y[train_indices_EVaR_ℓ1]\n",
    "                X_val_EVaR_ℓ1, Y_val_EVaR_ℓ1 = X[val_indices_EVaR_ℓ1, :], Y[val_indices_EVaR_ℓ1]\n",
    "                MSE_EVaR_ℓ1 = calculate_MSE(X_val_EVaR_ℓ1, Y_val_EVaR_ℓ1, w_opt_EVaR_ℓ1)\n",
    "                if MSE_EVaR_ℓ1 < best_MSE_EVaR_ℓ1\n",
    "                    best_λ_EVaR_ℓ1 = λ_EVaR_ℓ1\n",
    "                    best_c_s_EVaR_ℓ1 = c_s_EVaR_ℓ1\n",
    "                    best_c_w_EVaR_ℓ1 = c_w_EVaR_ℓ1\n",
    "                    best_MSE_EVaR_ℓ1 = MSE_EVaR_ℓ1\n",
    "                    println(\"λ=\", λ_EVaR_ℓ1, \"\\t c_s_EVaR_ℓ1=\", c_s_EVaR_ℓ1, \"\\t c_w_EVaR_ℓ1=\", c_w_EVaR_ℓ1, \"\\t MSE=\", MSE_EVaR_ℓ1)\n",
    "                else\n",
    "                    println(λ_EVaR_ℓ1, \"    \",c_s_EVaR_ℓ1, \"    \", c_w_EVaR_ℓ1)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    println(\"Best λ_EVaR_ℓ1 = \", best_λ_EVaR_ℓ1, \"\\t c_s_EVaR_ℓ1=\", best_c_s_EVaR_ℓ1, \"\\t c_w_EVaR_ℓ1=\", best_c_w_EVaR_ℓ1)\n",
    "\n",
    "    # Fitting Stable regression with best λ\n",
    "    list_f_opt, s_opt, w_opt_EVaR_ℓ1 = gradient_descent_EVaR_ℓ1(\n",
    "        X, Y, s_0, w_0, α, best_λ_EVaR_ℓ1, best_c_s_EVaR_ℓ1, best_c_w_EVaR_ℓ1, ε\n",
    "    )\n",
    "    train_indices_EVaR_ℓ1, val_indices_EVaR_ℓ1 = get_training_and_validation_indices(\n",
    "        X, Y, w_opt_EVaR_ℓ1, true, k\n",
    "    )\n",
    "    X_train_EVaR_ℓ1, Y_train_EVaR_ℓ1 = X[train_indices_EVaR_ℓ1, :], Y[train_indices_EVaR_ℓ1]\n",
    "    X_val_EVaR_ℓ1, Y_val_EVaR_ℓ1 = X[val_indices_EVaR_ℓ1, :], Y[val_indices_EVaR_ℓ1]\n",
    "    println(\"Fitted EVaR_ℓ1egression ℓ_1 with best possible λ_EVaR_ℓ1=\", best_λ_EVaR_ℓ1, \n",
    "        \"  c_s_EVaR_ℓ1=\", best_c_s_EVaR_ℓ1, \"  c_w_EVaR_ℓ1=\", best_c_w_EVaR_ℓ1)\n",
    "    \n",
    "\n",
    "\n",
    "    println(\"For k = \", k,\", n = \",n)\n",
    "    println(\"########## The training scores are: ##########\")\n",
    "    println(\"The MSE for the EVaR_ℓ1 Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_train_EVaR_ℓ1, Y_train_EVaR_ℓ1, w_opt_EVaR_ℓ1))\n",
    "    println(\"The MSE for the EVaR Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_train_EVaR, Y_train_EVaR, w_opt_EVaR))\n",
    "\n",
    "\n",
    "    println(\"########## The validation scores are: ##########\")\n",
    "    println(\"The MSE for the EVaR_ℓ1 Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_val_EVaR_ℓ1, Y_val_EVaR_ℓ1, w_opt_EVaR_ℓ1))\n",
    "    println(\"The MSE for the EVaR Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_val_EVaR, Y_val_EVaR, w_opt_EVaR))\n",
    "\n",
    "\n",
    "    println(\"########## The test scores are: ##########\")\n",
    "    println(\"The MSE for the EVaR_ℓ1 Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_test, Y_test, w_opt_EVaR_ℓ1))\n",
    "    println(\"The MSE for the EVaR Regression is for ε = 0.001 : \", \n",
    "        calculate_MSE(X_test, Y_test, w_opt_EVaR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
